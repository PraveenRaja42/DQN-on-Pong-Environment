{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DQN(\n",
      "  (conv): Sequential(\n",
      "    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
      "    (3): ReLU()\n",
      "    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (5): ReLU()\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=3136, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=6, bias=True)\n",
      "  )\n",
      ")\n",
      "762: done 1 games, mean reward -21.000, eps 0.99, speed 216.04 f/s\n",
      "1621: done 2 games, mean reward -20.500, eps 0.98, speed 311.23 f/s\n",
      "Best mean reward updated -21.000 -> -20.500, model saved\n",
      "2593: done 3 games, mean reward -20.667, eps 0.97, speed 307.04 f/s\n",
      "3492: done 4 games, mean reward -20.750, eps 0.97, speed 291.43 f/s\n",
      "4254: done 5 games, mean reward -20.800, eps 0.96, speed 280.57 f/s\n",
      "5076: done 6 games, mean reward -20.833, eps 0.95, speed 301.45 f/s\n",
      "6033: done 7 games, mean reward -20.857, eps 0.94, speed 299.26 f/s\n",
      "6995: done 8 games, mean reward -20.875, eps 0.93, speed 290.92 f/s\n",
      "7907: done 9 games, mean reward -20.889, eps 0.92, speed 290.95 f/s\n",
      "8943: done 10 games, mean reward -20.700, eps 0.91, speed 277.67 f/s\n",
      "9860: done 11 games, mean reward -20.636, eps 0.90, speed 286.72 f/s\n",
      "10684: done 12 games, mean reward -20.667, eps 0.89, speed 39.61 f/s\n",
      "11696: done 13 games, mean reward -20.615, eps 0.88, speed 32.58 f/s\n",
      "12610: done 14 games, mean reward -20.643, eps 0.87, speed 32.47 f/s\n",
      "13542: done 15 games, mean reward -20.533, eps 0.86, speed 33.61 f/s\n",
      "14490: done 16 games, mean reward -20.500, eps 0.86, speed 32.76 f/s\n",
      "15514: done 17 games, mean reward -20.412, eps 0.84, speed 32.17 f/s\n",
      "Best mean reward updated -20.500 -> -20.412, model saved\n",
      "16336: done 18 games, mean reward -20.444, eps 0.84, speed 30.05 f/s\n",
      "17176: done 19 games, mean reward -20.421, eps 0.83, speed 31.75 f/s\n",
      "18166: done 20 games, mean reward -20.450, eps 0.82, speed 31.41 f/s\n",
      "19105: done 21 games, mean reward -20.476, eps 0.81, speed 32.20 f/s\n",
      "20018: done 22 games, mean reward -20.455, eps 0.80, speed 32.44 f/s\n",
      "20901: done 23 games, mean reward -20.478, eps 0.79, speed 34.24 f/s\n",
      "21725: done 24 games, mean reward -20.500, eps 0.78, speed 33.94 f/s\n",
      "22783: done 25 games, mean reward -20.440, eps 0.77, speed 34.14 f/s\n",
      "23726: done 26 games, mean reward -20.423, eps 0.76, speed 33.70 f/s\n",
      "24684: done 27 games, mean reward -20.407, eps 0.75, speed 34.53 f/s\n",
      "Best mean reward updated -20.412 -> -20.407, model saved\n",
      "25508: done 28 games, mean reward -20.429, eps 0.74, speed 33.12 f/s\n",
      "26425: done 29 games, mean reward -20.414, eps 0.74, speed 34.48 f/s\n",
      "27335: done 30 games, mean reward -20.433, eps 0.73, speed 32.30 f/s\n",
      "28332: done 31 games, mean reward -20.419, eps 0.72, speed 33.88 f/s\n",
      "29321: done 32 games, mean reward -20.406, eps 0.71, speed 34.14 f/s\n",
      "Best mean reward updated -20.407 -> -20.406, model saved\n",
      "30172: done 33 games, mean reward -20.424, eps 0.70, speed 33.55 f/s\n",
      "31329: done 34 games, mean reward -20.353, eps 0.69, speed 33.96 f/s\n",
      "Best mean reward updated -20.406 -> -20.353, model saved\n",
      "32138: done 35 games, mean reward -20.371, eps 0.68, speed 33.65 f/s\n",
      "33272: done 36 games, mean reward -20.306, eps 0.67, speed 33.89 f/s\n",
      "Best mean reward updated -20.353 -> -20.306, model saved\n",
      "34141: done 37 games, mean reward -20.324, eps 0.66, speed 33.96 f/s\n",
      "35302: done 38 games, mean reward -20.289, eps 0.65, speed 33.71 f/s\n",
      "Best mean reward updated -20.306 -> -20.289, model saved\n",
      "36307: done 39 games, mean reward -20.282, eps 0.64, speed 33.17 f/s\n",
      "Best mean reward updated -20.289 -> -20.282, model saved\n",
      "37296: done 40 games, mean reward -20.275, eps 0.63, speed 32.85 f/s\n",
      "Best mean reward updated -20.282 -> -20.275, model saved\n",
      "38327: done 41 games, mean reward -20.244, eps 0.62, speed 33.73 f/s\n",
      "Best mean reward updated -20.275 -> -20.244, model saved\n",
      "39420: done 42 games, mean reward -20.190, eps 0.61, speed 33.32 f/s\n",
      "Best mean reward updated -20.244 -> -20.190, model saved\n",
      "40321: done 43 games, mean reward -20.186, eps 0.60, speed 32.67 f/s\n",
      "Best mean reward updated -20.190 -> -20.186, model saved\n",
      "41688: done 44 games, mean reward -20.091, eps 0.58, speed 33.01 f/s\n",
      "Best mean reward updated -20.186 -> -20.091, model saved\n",
      "43074: done 45 games, mean reward -20.000, eps 0.57, speed 33.85 f/s\n",
      "Best mean reward updated -20.091 -> -20.000, model saved\n",
      "44228: done 46 games, mean reward -20.000, eps 0.56, speed 33.23 f/s\n",
      "45535: done 47 games, mean reward -20.000, eps 0.54, speed 32.67 f/s\n",
      "46659: done 48 games, mean reward -20.000, eps 0.53, speed 17.10 f/s\n",
      "47844: done 49 games, mean reward -20.000, eps 0.52, speed 18.09 f/s\n",
      "49123: done 50 games, mean reward -19.980, eps 0.51, speed 17.91 f/s\n",
      "Best mean reward updated -20.000 -> -19.980, model saved\n",
      "50205: done 51 games, mean reward -19.961, eps 0.50, speed 18.11 f/s\n",
      "Best mean reward updated -19.980 -> -19.961, model saved\n",
      "51288: done 52 games, mean reward -19.942, eps 0.49, speed 17.85 f/s\n",
      "Best mean reward updated -19.961 -> -19.942, model saved\n",
      "52709: done 53 games, mean reward -19.943, eps 0.47, speed 18.04 f/s\n",
      "54098: done 54 games, mean reward -19.926, eps 0.46, speed 17.86 f/s\n",
      "Best mean reward updated -19.942 -> -19.926, model saved\n",
      "55433: done 55 games, mean reward -19.873, eps 0.45, speed 17.90 f/s\n",
      "Best mean reward updated -19.926 -> -19.873, model saved\n",
      "56715: done 56 games, mean reward -19.839, eps 0.43, speed 17.87 f/s\n",
      "Best mean reward updated -19.873 -> -19.839, model saved\n",
      "58470: done 57 games, mean reward -19.789, eps 0.42, speed 18.02 f/s\n",
      "Best mean reward updated -19.839 -> -19.789, model saved\n",
      "59934: done 58 games, mean reward -19.793, eps 0.40, speed 17.76 f/s\n",
      "61623: done 59 games, mean reward -19.780, eps 0.38, speed 17.78 f/s\n",
      "Best mean reward updated -19.789 -> -19.780, model saved\n",
      "63159: done 60 games, mean reward -19.800, eps 0.37, speed 17.71 f/s\n",
      "64483: done 61 games, mean reward -19.820, eps 0.36, speed 17.76 f/s\n",
      "66130: done 62 games, mean reward -19.758, eps 0.34, speed 17.68 f/s\n",
      "Best mean reward updated -19.780 -> -19.758, model saved\n",
      "67940: done 63 games, mean reward -19.683, eps 0.32, speed 17.50 f/s\n",
      "Best mean reward updated -19.758 -> -19.683, model saved\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-39484d7c2220>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    176\u001b[0m         \u001b[0mloss_t\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalc_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtgt_net\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m         \u001b[0mloss_t\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 178\u001b[1;33m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    179\u001b[0m     \u001b[0mwriter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m     98\u001b[0m                 \u001b[1;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m                 \u001b[0mexp_avg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 100\u001b[1;33m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    101\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m                     \u001b[1;31m# Maintains the maximum of all 2nd moment running avg. till now\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import wrappers\n",
    "import dqn_model\n",
    "\n",
    "import argparse\n",
    "import time\n",
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "\n",
    "DEFAULT_ENV_NAME = \"PongNoFrameskip-v4\"\n",
    "MEAN_REWARD_BOUND = 19.5\n",
    "\n",
    "GAMMA = 0.99\n",
    "BATCH_SIZE = 32\n",
    "REPLAY_SIZE = 10000\n",
    "LEARNING_RATE = 1e-4\n",
    "SYNC_TARGET_FRAMES = 1000\n",
    "REPLAY_START_SIZE = 10000\n",
    "\n",
    "EPSILON_DECAY_LAST_FRAME = 10**5\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_FINAL = 0.02\n",
    "\n",
    "\n",
    "Experience = collections.namedtuple('Experience', field_names=['state', 'action', 'reward', 'done', 'new_state'])\n",
    "\n",
    "\n",
    "class ExperienceBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = collections.deque(maxlen=capacity)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def append(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
    "        states, actions, rewards, dones, next_states = zip(*[self.buffer[idx] for idx in indices])\n",
    "        return np.array(states), np.array(actions), np.array(rewards, dtype=np.float32), \\\n",
    "               np.array(dones, dtype=np.uint8), np.array(next_states)\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, env, exp_buffer):\n",
    "        self.env = env\n",
    "        self.exp_buffer = exp_buffer\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self):\n",
    "        self.state = env.reset()\n",
    "        self.total_reward = 0.0\n",
    "\n",
    "    def play_step(self, net, epsilon=0.0, device=\"cpu\"):\n",
    "        done_reward = None\n",
    "\n",
    "        if np.random.random() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            state_a = np.array([self.state], copy=False)\n",
    "            state_v = torch.tensor(state_a).to(device)\n",
    "            q_vals_v = net(state_v)\n",
    "            _, act_v = torch.max(q_vals_v, dim=1)\n",
    "            action = int(act_v.item())\n",
    "\n",
    "        # do step in the environment\n",
    "        new_state, reward, is_done, _ = self.env.step(action)\n",
    "        self.total_reward += reward\n",
    "\n",
    "        exp = Experience(self.state, action, reward, is_done, new_state)\n",
    "        self.exp_buffer.append(exp)\n",
    "        self.state = new_state\n",
    "        if is_done:\n",
    "            done_reward = self.total_reward\n",
    "            self._reset()\n",
    "        return done_reward\n",
    "\n",
    "\n",
    "def calc_loss(batch, net, tgt_net, device=\"cpu\"):\n",
    "    states, actions, rewards, dones, next_states = batch\n",
    "\n",
    "    states_v = torch.tensor(states).to(device)\n",
    "    next_states_v = torch.tensor(next_states).to(device)\n",
    "    actions_v = torch.tensor(actions).to(device=device, dtype=torch.int64)\n",
    "    rewards_v = torch.tensor(rewards).to(device)\n",
    "    done_mask = torch.tensor(dones).to(device=device, dtype=torch.bool)\n",
    "\n",
    "    state_action_values = net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)\n",
    "    next_state_values = tgt_net(next_states_v).max(1)[0]\n",
    "    next_state_values[done_mask] = 0.0\n",
    "    next_state_values = next_state_values.detach()\n",
    "\n",
    "    expected_state_action_values = next_state_values * GAMMA + rewards_v\n",
    "    return nn.MSELoss()(state_action_values, expected_state_action_values)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #parser = argparse.ArgumentParser()\n",
    "    #parser.add_argument(\"--cuda\", default=False, action=\"store_true\", help=\"Enable cuda\")\n",
    "    #parser.add_argument(\"--env\", default=DEFAULT_ENV_NAME,\n",
    "                        #help=\"Name of the environment, default=\" + DEFAULT_ENV_NAME)\n",
    "    #parser.add_argument(\"--reward\", type=float, default=MEAN_REWARD_BOUND,\n",
    "                        #help=\"Mean reward boundary for stop of training, default=%.2f\" % MEAN_REWARD_BOUND)\n",
    "    #args = parser.parse_args()\n",
    "    \n",
    "    import easydict\n",
    "    args = easydict.EasyDict({\n",
    "            \"cuda\": True,\n",
    "            \"env\": DEFAULT_ENV_NAME,\n",
    "            \"reward\": MEAN_REWARD_BOUND,\n",
    "    })\n",
    "    \n",
    "    use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "    \n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    \n",
    "    env = wrappers.make_env(args.env)\n",
    "\n",
    "    net = dqn_model.DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
    "    tgt_net = dqn_model.DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
    "    writer = SummaryWriter(comment=\"-\" + args.env)\n",
    "    print(net)\n",
    "\n",
    "    buffer = ExperienceBuffer(REPLAY_SIZE)\n",
    "    agent = Agent(env, buffer)\n",
    "    epsilon = EPSILON_START\n",
    "\n",
    "    optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
    "    total_rewards = []\n",
    "    frame_idx = 0\n",
    "    ts_frame = 0\n",
    "    ts = time.time()\n",
    "    best_mean_reward = None\n",
    "\n",
    "    while True:\n",
    "        frame_idx += 1\n",
    "        epsilon = max(EPSILON_FINAL, EPSILON_START - frame_idx / EPSILON_DECAY_LAST_FRAME)\n",
    "\n",
    "        reward = agent.play_step(net, epsilon, device=device)\n",
    "        if reward is not None:\n",
    "            total_rewards.append(reward)\n",
    "            speed = (frame_idx - ts_frame) / (time.time() - ts)\n",
    "            ts_frame = frame_idx\n",
    "            ts = time.time()\n",
    "            mean_reward = np.mean(total_rewards[-100:])\n",
    "            print(\"%d: done %d games, mean reward %.3f, eps %.2f, speed %.2f f/s\" % (\n",
    "                frame_idx, len(total_rewards), mean_reward, epsilon,\n",
    "                speed\n",
    "            ))\n",
    "            writer.add_scalar(\"epsilon\", epsilon, frame_idx)\n",
    "            writer.add_scalar(\"speed\", speed, frame_idx)\n",
    "            writer.add_scalar(\"reward_100\", mean_reward, frame_idx)\n",
    "            writer.add_scalar(\"reward\", reward, frame_idx)\n",
    "            if best_mean_reward is None or best_mean_reward < mean_reward:\n",
    "                torch.save(net.state_dict(), args.env + \"-best.dat\")\n",
    "                if best_mean_reward is not None:\n",
    "                    print(\"Best mean reward updated %.3f -> %.3f, model saved\" % (best_mean_reward, mean_reward))\n",
    "                best_mean_reward = mean_reward\n",
    "            if mean_reward > args.reward:\n",
    "                print(\"Solved in %d frames!\" % frame_idx)\n",
    "                break\n",
    "\n",
    "        if len(buffer) < REPLAY_START_SIZE:\n",
    "            continue\n",
    "\n",
    "        if frame_idx % SYNC_TARGET_FRAMES == 0:\n",
    "            tgt_net.load_state_dict(net.state_dict())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        batch = buffer.sample(BATCH_SIZE)\n",
    "        loss_t = calc_loss(batch, net, tgt_net, device=device)\n",
    "        loss_t.backward()\n",
    "        optimizer.step()\n",
    "    writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
